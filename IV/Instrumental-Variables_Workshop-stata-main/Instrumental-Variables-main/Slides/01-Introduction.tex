\documentclass{beamer}

\input{preamble.tex}

\begin{document}

\imageframe{./lecture_includes/cover_intro.png}

\section{Introductions}

\subsection{Who Am I?}
\begin{frame}{Who Am I?}

Groos Family Assistant Professor of Economics, Brown University\pause

A big fan of instrumental variable (IV) methods\pause
\begin{itemize}
  \item Lottery- and non-lottery IVs in studies of educational quality \\ {\scriptsize \textcolor{red!75!green!50!blue!25!gray}{(Angrist et al. 2016, 2017, 2021, 2022; Abdulkadiro\u{g}lu et al. 2016)}}
  \item Quasi-experimental evaluations of healthcare quality \\ {\scriptsize \textcolor{red!75!green!50!blue!25!gray}{(Hull 2020; Abaluck et al. 2021, 2022)}}
  \item IV-based analyses of discrimination and bias \\ {\scriptsize \textcolor{red!75!green!50!blue!25!gray}{(Arnold et al. 2020, 2021, 2022; Hull 2021; Bohren et al. 2022)}}
  \item Shift-share instruments and related designs \\ {\scriptsize \textcolor{red!75!green!50!blue!25!gray}{(Borusyak et al. 2022; Borusyak and Hull 2021, 2022; Goldsmith-Pinkham et al. 2022)}}
\end{itemize}\pause

A constant student of IV (and econometrics more generally)

\end{frame}

\subsection{What is This Course?}
\begin{frame}{What is This Course?}

A two-day intensive on IV, focusing on recent practical advances \pause

\begin{itemize}
  \item Far from comprehensive --- stay tuned for more ``mixtape tracks'' that take deeper dives on particular topics (judge IV, shift-share, etc)
  \item Emphasis on \emph{practical}: IV is meant to be used, not just studied!
\end{itemize}\pause\medskip

Four one-hour lectures: from IV basics to recent topics

\begin{itemize}
  \item Please ask questions in the Discord chat!
  \item I will try to stick to the schedule but may improvise slightly
\end{itemize}\pause\medskip

Two 40-minute coding labs, applying what we've learned
\begin{itemize}
  \item 20 min: you seeing how far you can get on your own, or with your classmate's help (use Discord rooms!)
  \item 20 min: me live-coding solutions in Stata (we will also post R code)
\end{itemize}

\end{frame}

\begin{frame}{Schedule}
\includegraphics[scale=0.58]{./lecture_includes/schedule.png}
\end{frame}

\section{Regression Review}

\subsection{Models vs. Estimands vs. Estimators}
\begin{frame}{Models vs. Estimands vs. Estimators}

Three distinct objects (though not always clearly distinguished) \pause\medskip
{\small
\begin{itemize}
  \item \bgVioletRed{Parameters} come from models of how observed data are generated
  \begin{itemize}\itemsep0em
    \item E.g. a structural supply/demand elasticity, an ATE, or a CEF
    \item They set the target for an empirical analysis: what we want to know
  \end{itemize}\pause
  
  \item \bgPictonBlue{Estimands} are functions of the population data distribution 
  \begin{itemize}\itemsep0em
    \item E.g. a difference in means or ratio of population regression coef's
    \item Make assumptions to link parameters \& estimands (``identification'')
  \end{itemize}\pause
  
  \item \bgSun{Estimators} are functions of the observed data itself (the ``sample'')
  \begin{itemize}\itemsep0em
    \item E.g. a difference in sample means or ratio of OLS coefficients 
    \item Since data are random, so are estimators. Each has a distribution
    \item Use knowledge of estimator distributions to learn about estimands (``inference'') and---hopefully---identified parameters
  \end{itemize}
\end{itemize}
}
\end{frame}

\begin{frame}{The Lay of the Land}
  \begin{adjustbox}{width = 0.9\textwidth, center}
    \begin{tikzpicture}
      \node[violet-red] (theory) at (-3.5, 1.25) {Economic Theory};
      \node[state,rectangle,violet-red] (parameters) at (-3.5, 0) {Parameters};
      
      \node[picton-blue] (distribution) at (0, 1.25) {Data Distribution};
      \node[state,rectangle,picton-blue] (estimands) at (0, 0) {Estimands};

      \node[sun] (data) at (3.5, 1.25) {Observed Datasets};
      \node[state,rectangle,sun] (estimators) at (3.5, 0) {Estimators};

      \path[violet-red, thick, shorten >=4pt] (theory) edge (parameters);
      \path[picton-blue, thick, shorten >=4pt] (distribution) edge (estimands);
      \path[sun, thick, shorten >=4pt] (data) edge (estimators);

      \path[thick, shorten >=4pt, shorten <=4pt] (estimands) edge[bend left= 40] node[below, el, yshift=-4pt] {Identification} (parameters);
      \path[thick, shorten >=4pt, shorten <=4pt] (estimators) edge[bend left= 40] node[below, el, yshift=-4pt] {Statistical Inference} (estimands);
    \end{tikzpicture}
  \end{adjustbox}

  \vspace{10mm}
  This course will mostly focus on identification, but we'll cover some IV estimation / inference issues as well
\end{frame}

\subsection{Regression Identification and Endogeneity}
\begin{frame}{Let's Get Specific}
Human capital theory (e.g. Becker, 1957) tells us that taking two-day IV intensives are likely to boost later-life productivity\pause
\begin{itemize}
  \item Parameter: returns to taking this class $\violetRed{\beta}$, measured in some outcome $Y_i$ (e.g. lifetime top-5 pubs / earnings / twitter followers)
  \item Simple causal/structural model: $Y_i= \alpha + \violetRed{\beta} D_i + \varepsilon_i$, where $D_i \in \{0,1\}$ indicates taking this class
\end{itemize}\pause

We see a sample of $Y_i$, $D_i$, and some other covariates $W_{1i},\dots,W_{Ki}$
\begin{itemize}
  \item We fire up Stata and type \code{reg y d w*, r}
\item How do we interpret the output?
\end{itemize}

\end{frame}

\begin{frame}{Population Regression}
  The OLS \emph{estimator} $\sun{\widehat{\beta}^{OLS}}$ consistently estimates the regression \emph{estimand} $\pictonBlue{\beta^{OLS}}$ under relatively weak conditions (e.g. \emph{i.i.d.} data)
  \begin{itemize}
  \item Stata tells us $\sun{\widehat{\beta}^{OLS}}$ and what we can infer about $\pictonBlue{\beta^{OLS}}$ from it
  \item It \emph{doesn't} directly tell us about the relationship between $\pictonBlue{\beta^{OLS}}$ and $\violetRed{\beta}$
  \end{itemize}
\end{frame}

\begin{frame}{Population Regression}

The \bgPictonBlue{population regression} of $Y_i$ on $\mathbf{X}_i=[1, D_i,W_{1i},\dots,W_{Ki}]^\prime$ is given by $Y_i=\mathbf{X}_i^\prime \pictonBlue{\beta^{OLS}} + U_i$ where $E[\mathbf{X}_i U_i]=0$\pause
\begin{itemize}
  \item Equivalently, $\pictonBlue{\beta^{OLS}} = E[\mathbf{X}_i\mathbf{X}_i^\prime]^{-1}E[\mathbf{X}_i Y_i]$ and $U_i = Y_i - \mathbf{X}_i^\prime \pictonBlue{\beta^{OLS}} $\smallskip
\item $\pictonBlue{\beta^{OLS}} $ contains regression \emph{coefficients}; $U_i$ is the regression \emph{residual} 
\end{itemize}\medskip\pause


Key point: we can always define $\pictonBlue{\beta^{OLS}} $ for any $Y_i$ and $\mathbf{X}_i$ (assuming no perfect collinearity); this is what Stata estimates
\begin{itemize}
  \item Specifically it computes $\sun{\widehat{\beta}^{OLS}} = (\frac{1}{N}\sum_i\mathbf{X}_i\mathbf{X}_i^\prime)^{-1}(\frac{1}{N}\sum_i\mathbf{X}_iY_i)$ and uses large-sample asymptotics (LLN/CLT) to get a standard error
\end{itemize}

\end{frame}

\begin{frame}{You Can't Always Get What you Want...}
But what if this \emph{estimand} is not what we want? \pause
\begin{itemize}
  \item What if $\pictonBlue{\beta^{OLS}}$ fails to coincide with our economic parameter of interest (e.g. returns to mixtape workshops)?
\end{itemize}
\end{frame}



\begin{frame}{You Can't Always Get What you Want...}

The model parameter in $Y_i=\alpha+ \violetRed{\beta} D_i+\varepsilon_i$ need not coincide with the regression coefficient in $Y_i= \alpha^{OLS} + \pictonBlue{\beta^{OLS}} D_i+U_i$
\begin{itemize}
  \item I.e. we may not have $Cov(D_i,\varepsilon_i)=0$ (always have $Cov(D_i,U_i)=0)$
\end{itemize}\bigskip\pause

Selection bias (a.k.a. omitted variables bias): students with higher latent earnings potential $\varepsilon_i$ are more likely to take this class $D_i$
\begin{itemize}
  \item $Cov(D_i,\varepsilon_i)>0$ means $\pictonBlue{\beta^{OLS}} > \violetRed{\beta}$: overstate the returns-to-mixtape
\end{itemize}\bigskip
\end{frame}

\begin{frame}{Can I just Control My Way Out of This?}

Adding more controls (e.g. demographics) may or may not help
\begin{itemize}
  \item Projecting $\varepsilon_i$ on $X_i$, we get $Y_i=\alpha+ \violetRed{\beta} D_i + \gamma X_i + \tilde\varepsilon_i$, $Cov(X_i,\tilde\varepsilon_i)=0$
  \item Whether or not $Cov(D_i,\tilde\varepsilon_i)=0$ depends on whether $X_i$ sufficiently accounts for the confounding relationship $Cov(D_i,\varepsilon_i)\neq 0$
\end{itemize}
\end{frame}

\begin{frame}{Regression ``Exogeneity''}
  \begin{adjustbox}{width = 0.6\textwidth, center}
    \begin{tikzpicture}
      \node[state] (D) at (0,0) {$D$};
      \node[state] (Y) at (2,0) {$Y$};
      \node[state, ruby] (epsilon) at (1,1.5) {$\epsilon$};
    
      \path[violet-red, thick] (D) edge node[above, el] {$\violetRed{\beta}$} (Y);
      \path[ruby, thick] (epsilon) edge (Y);
    \end{tikzpicture}
  \end{adjustbox}
\end{frame}

\begin{frame}{Regression ``Endogeneity''}
  \begin{adjustbox}{width = 0.6\textwidth, center}
    \begin{tikzpicture}
      \node[state] (D) at (0,0) {$D$};
      \node[state] (Y) at (2,0) {$Y$};
      \node[state, ruby] (epsilon) at (1,1.5) {$\epsilon$};
    
      \path[violet-red, thick] (D) edge node[above, el] {$\violetRed{\beta}$} (Y);
      \path[ruby, thick] (epsilon) edge (Y);
      \path[ruby, thick] (epsilon) edge (D);
    \end{tikzpicture}
  \end{adjustbox}
\end{frame}


\begin{frame}{...But Sometimes, You Get What you Need}

Imagine this course was ``oversubscribed,'' and admission was determined by lottery
\begin{itemize}
  \item Among those interested in taking the course, a random sample denoted by $Z_i = 1$ was given access
  \item The rest, with $Z_i = 0$ not initially given access (maybe got in later)
\end{itemize}\medskip\pause

Intuitively, this external shock $Z_i$ should be helpful for identifying $\violetRed{\beta}$
\begin{itemize}
  \item Affects $D_i$, so relevant to the ``treatment'' of interest
  \item Randomly assigned, so unconfounded by selection (unlike $D_i$)
\end{itemize}\medskip\pause

Indeed, this leads us to IV estimands (and estimators)

\end{frame}

\begin{frame}{The IV Solution}
  \begin{adjustbox}{width = 0.7\textwidth, center}
    \begin{tikzpicture}
      \node[state] (D) at (0,0) {$D$};
      \node[state] (Y) at (2,0) {$Y$};
      \node[state, ruby] (epsilon) at (1,1.5) {$\epsilon$};
      \node[state, kelly] (Z) at (-2,0) {$Z$};
    
      \path[violet-red, thick] (D) edge node[above, el] {$\violetRed{\beta}$} (Y);
      \path[ruby, thick] (epsilon) edge (Y);
      \path[ruby, thick] (epsilon) edge (D);
      \path[kelly, thick] (Z) edge (D);
    \end{tikzpicture}
  \end{adjustbox}
\end{frame}

\section{Introduction to IV}

\subsection{Instrument Validity and Relevance}
\begin{frame}{Instrument Validity and Relevance}

Causal/structural model $Y_i=\alpha+ \violetRed{\beta} D_i+\varepsilon_i$ and a candidate IV $Z_i$ 
\begin{itemize}
  \item Single $D_i$ and $Z_i$ and no further controls, for now
\end{itemize}\medskip\pause

Two key assumptions:
\begin{itemize}
  \item Relevance: $Z_i$ and $D_i$ are correlated: $Cov(Z_i,D_i)\neq 0$ 
  \item Validity: $Z_i$ and $\varepsilon_i$ are \emph{un}correlated: $Cov(Z_i,\varepsilon_i)=0$
\end{itemize}\medskip\pause

We then have identification:\vspace{-0.3cm}
\begin{align*}
  Cov(Z_i,Y_i) &= Cov(Z_i,\alpha + \violetRed{\beta} D_i+\varepsilon_i) \pause = \violetRed{\beta} Cov(Z_i,D_i)+Cov(Z_i,\varepsilon_i) \\ \pause
  &= \violetRed{\beta} Cov(Z_i,D_i) \pause \implies \violetRed{\beta} = \frac{Cov(Z_i,Y_i)}{Cov(Z_i,D_i)}
\end{align*}

\end{frame}

\begin{frame}{The IV Estimand}
The \bgPictonBlue{(simple) IV \emph{estimand}} is:
$$\pictonBlue{\beta^{IV}} = \frac{Cov(Z_i,Y_i)}{Cov(Z_i,D_i)}$$ 
\pause

\begin{itemize}
  \item Compare to the OLS estimand: $\pictonBlue{\beta^{OLS}}=\frac{Cov(D_i,Y_i)}{Var(D_i)}$ 
\end{itemize}

\end{frame}

\begin{frame}{``Reduced Form'' and ``First Stage''}

Note that we can write:
\begin{align*}
\pictonBlue{\beta^{IV}} = \frac{Cov(Z_i,Y_i)}{Cov(Z_i,D_i)} \pause = \frac{Cov(Z_i,Y_i)/Var(Z_i)}{Cov(Z_i,D_i)/Var(Z_i)} \pause = \frac{\pictonBlue{\rho^{OLS}}}{\pictonBlue{\pi^{OLS}}}
\end{align*}
where $\pictonBlue{\rho^{OLS}}$ and $\pictonBlue{\pi^{OLS}}$ are two OLS estimands:\pause
\begin{align*}
Y_i &= \kappa^{OLS} + \pictonBlue{\rho^{OLS}} Z_i + V_i \hspace{0.2cm}\text{ ``reduced form''}\\
D_i &= \mu^{OLS} + \pictonBlue{\pi^{OLS}} Z_i + W_i\hspace{0.2cm}\text{ ``first stage''}
\end{align*}

\end{frame}

\begin{frame}{IV estimand as the ``Second Stage''}

Sometimes we refer to the IV estimand as the ``second stage'':
\begin{align*}
Y_i=\alpha^{IV}+ \pictonBlue{\beta^{IV}} D_i+ U_i
\end{align*}

where now $Cov(Z_i,U_i)=0$. Thus ``IV=RF/FS'' ($\pictonBlue{\beta^{IV}} =\pictonBlue{\rho^{OLS}}/\pictonBlue{\pi^{OLS}}$)

\end{frame}

\subsection{The 2SLS Estimator}
\begin{frame}{The 2SLS Estimator}

  As with OLS, we estimate IV by sample analog:
\begin{align*}
\sun{\widehat\beta^{IV}} = \frac{\widehat{Cov}(Z_i,Y_i)}{\widehat{Cov}(Z_i,D_i)} = \frac{\sun{\widehat\rho^{OLS}}}{\sun{\widehat\pi^{OLS}}}
\end{align*}
where $\widehat{Cov}(X_i,W_i)=\frac{1}{N}\sum_i X_iW_i-\left(\frac{1}{N}\sum_i X_i\right)\left(\frac{1}{N}\sum_i W_i\right)$, $\hat\rho^{OLS}=\widehat{Cov}(Z_i,Y_i)/\widehat{Var}(Z_i)$, and $\hat\pi^{OLS}=\widehat{Cov}(Z_i,D_i)/\widehat{Var}(Z_i)$

\begin{itemize}\pause
\item This is what Stata does when you type \code{ivreg2 y (d=z), r}
\item Standard errors come from the usual large-sample asymptotics
\end{itemize}\bigskip\pause

We will soon consider extensions of all of this, with controls / multiple instruments / etc
\end{frame}

\begin{frame}{Angrist (1990): The ``Draft Lottery Paper''}

Angrist famously used Vietnam-era draft eligibility as an instrument to estimate the earnings effects of military service 
\begin{itemize}
\item Let $Z_i\in\{0,1\}$ be an indicator for draft eligibility, $D_i\in\{0,1\}$ be an indicator for military service, and $Y_i$ measure later-life earnings
\end{itemize}\medskip\pause

Here $\pictonBlue{\beta^{IV}} = \frac{Cov(Z_i,Y_i)/Var(Z_i)}{Cov(Z_i,D_i)/Var(Z_i)} = \frac{E[Y_i\mid Z_i=1]-E[Y_i\mid Z_i=0]}{E[D_i\mid Z_i=1]-E[D_i\mid Z_i=0]}$ has a special name, because $Z_i$ is binary: the \bgPictonBlue{Wald estimand}\pause{}
\begin{itemize}
\item First stage $E[D_i\mid Z_i=1]-E[D_i\mid Z_i=0]$: effect of eligibility on the \emph{probability} of military service (b/c $D_i$ is binary)
\item Reduced form $E[Y_i\mid Z_i=1]-E[Y_i\mid Z_i=0]$: effect of eligibility on adult earnings (measured in 1971, 1981...)
\end{itemize}\medskip\pause

IV interprets the latter causal effect in terms of the former
\end{frame}

\imageframe{./lecture_includes/angrist_1990.png}
\imageframe{./lecture_includes/angrist_1990_2.png}

\end{document}
